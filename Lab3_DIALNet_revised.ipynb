{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB3_DIALNet_revised.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Refactor the DialNet class so that instead of declaring DA layersr as separate components:\n",
        "\n",
        "  ```\n",
        "  self.bns1 = nn.batchNorm2d(64, affine=False)\n",
        "  self.bnt1 = nn.BatchNorm2d(64, affine=False)\n",
        "  self.gamma1 = nn.Parameter(torch.ones(64, 1, 1))\n",
        "  self.beta1 = nn.Parameter(torch.zeros(64, 1, 1))\n",
        "  ```\n",
        "\n",
        "  Defines them as self-contained DALayer2d or DALayer1d modules:\n",
        "\n",
        "  ```\n",
        "  self.da1 = DALayer2d(64)\n",
        "  ```"
      ],
      "metadata": {
        "id": "LqL-j-cHHymq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "uOlX6GGKO3rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DALayer2d(nn.Module):\n",
        "  def __init__(self, in_features):\n",
        "    super(DALayer2d, self).__init__()\n",
        "    self.in_features = in_features\n",
        "\n",
        "    self.batchnormsource = nn.BatchNorm2d(self.in_features, affine=False)\n",
        "    self.batchnormtarget = nn.BatchNorm2d(self.in_features, affine=False)\n",
        "    self.gamma = nn.parameter.Parameter(torch.ones(self.in_features, 1, 1))\n",
        "    self.beta = nn.parameter.Parameter(torch.zeros(self.in_features, 1, 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.training:\n",
        "      x_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "      return torch.cat((self.batchnormsource(x_source), self.batchnormtarget(x_target)), dim=0) * self.gamma + self.beta\n",
        "    else:\n",
        "      return self.batchnormtarget(x) * self.gamma + self.beta\n",
        "\n",
        "\n",
        "class DALayer1d(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "      super(DALayer1d, self).__init__()\n",
        "      self.in_features = in_features\n",
        "\n",
        "      self.batchnormsource = nn.BatchNorm1d(self.in_features, affine=False)\n",
        "      self.batchnormtarget = nn.BatchNorm1d(self.in_features, affine=False)\n",
        "      self.gamma = nn.parameter.Parameter(torch.ones(1, in_features))\n",
        "      self.beta = nn.parameter.Parameter(torch.zeros(1, in_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.training:\n",
        "        x_source, x_target = torch.split(x, split_size_or_sections=x.shape[0] // 2, dim=0)\n",
        "        return torch.cat((self.batchnormsource(x_source), self.batchnormtarget(x_target)), dim=0) * self.gamma + self.beta\n",
        "\n",
        "      else:\n",
        "        return self.batchnormtarget(x) * self.gamma + self.beta\n",
        "\n",
        "class DIALNet_rev(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(DIALNet_rev, self).__init__()\n",
        "    self.dial = nn.Sequential(nn.Conv2d(3, 64, kernel_size=5, padding=2),\n",
        "                   DALayer2d(64),\n",
        "                   nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "                   nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "                   DALayer2d(64),\n",
        "                   nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "                   nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "                   DALayer2d(128),\n",
        "                   nn.ReLU(),\n",
        "\n",
        "                   nn.Flatten(),\n",
        "\n",
        "                   nn.Linear(6272, 3072),\n",
        "                   DALayer1d(3072),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Dropout(),\n",
        "\n",
        "                   nn.Linear(3072, 2048),\n",
        "                   DALayer1d(2048),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Dropout(),\n",
        "\n",
        "                   nn.Linear(2048, 10),\n",
        "                   DALayer1d(10))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return self.dial(x)"
      ],
      "metadata": {
        "id": "taUPnpvKH5T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def source_loss():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "Ko3jZrKOWvxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_loss(x):\n",
        "  # Compute p_i\n",
        "  p = F.softmax(x, dim=1)\n",
        "  # Compute log p_i\n",
        "  q = F.log_softmax(x, dim=1)\n",
        "\n",
        "  b = p * q\n",
        "  b = -1.0 * b.sum(-1).mean()\n",
        "  return b"
      ],
      "metadata": {
        "id": "xx1OgE4_fZbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, source_loader, target_loader, optimizer, source_loss, entropy_loss_weights, device):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "\n",
        "  cumulative_source_loss = 0.\n",
        "  cumulative_target_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "\n",
        "  target_iter = iter(target_loader)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for (x_source, y) in source_loader:\n",
        "    # Gets target data. If the target iterator reaches the end, restarts it\n",
        "    try:\n",
        "      # if end of data is reached\n",
        "      x_target, _ = next(target_iter)\n",
        "    except:\n",
        "      # restart data loader\n",
        "      target_iter = iter(target_loader)\n",
        "      # iterate again\n",
        "      x_target, _ = next(target_iter)\n",
        "    \n",
        "    x = torch.cat((x_source, x_target), dim=0)\n",
        "\n",
        "    # Load into GPU\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    out = model(x)\n",
        "\n",
        "    # Split source and target outputs\n",
        "\n",
        "    source_y, target_y = torch.split(out,\n",
        "                                     split_size_or_sections=out.shape[0] // 2,\n",
        "                                     dim=0)\n",
        "\n",
        "    # Apply losses\n",
        "    sl = source_loss(source_y, y)\n",
        "    tl = target_loss(target_y)\n",
        "\n",
        "    loss = sl + entropy_loss_weights * tl\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Zeros the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    source_samples += x_source.shape[0]\n",
        "    target_samples += x_target.shape[0]\n",
        "\n",
        "    cumulative_source_loss += sl.item()\n",
        "    cumulative_target_loss += tl.item()\n",
        "\n",
        "    _, predicted = source_y.max(1)\n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "\n",
        "  return cumulative_source_loss/source_samples, cumulative_target_loss/target_samples, cumulative_accuracy/source_samples*100"
      ],
      "metadata": {
        "id": "7v6zYYkmfaJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_one_epoch(model, loader, cost_function, device):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      out = model(x)\n",
        "\n",
        "      loss = cost_function(out, y)\n",
        "\n",
        "      samples += x.shape[0]\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = out.max(dim=1)\n",
        "      cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "  \n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "hb0CUgu9fbJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "\n",
        "  transform_mnist = T.Compose([T.ToTensor(),\n",
        "                               T.Lambda(lambda x: F.pad(x, (2, 2, 2, 2), 'constant')),\n",
        "                               T.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "                               T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "  \n",
        "  transform_svhn = T.Compose([T.ToTensor(),\n",
        "                              T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "  \n",
        "  source_training_data = torchvision.datasets.SVHN('./data/svhn', split='train', transform=transform_svhn, download=True)\n",
        "\n",
        "  target_training_data = torchvision.datasets.MNIST('./data/mnist', train=True, transform=transform_mnist, download=True)\n",
        "  target_test_data = torchvision.datasets.MNIST('./data/mnist', train=False, transform=transform_mnist, download=True)\n",
        "  \n",
        "  # Init DataLoaders\n",
        "  source_train_loader = torch.utils.data.DataLoader(source_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "  target_train_loader = torch.utils.data.DataLoader(target_training_data, batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "  target_test_loader = torch.utils.data.DataLoader(target_test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "  return source_train_loader, target_train_loader, target_test_loader"
      ],
      "metadata": {
        "id": "y1UG7EjlfcGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(batch_size=32, device='cuda:0', learning_rate=1e-2, weight_decay=1e-6, epochs=25, entropy_loss_weight=0.1):\n",
        "\n",
        "  source_train_loader, target_train_loader, target_test_loader = get_data(batch_size=64)\n",
        "\n",
        "  model = DIALNet_rev().to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "  cost_function = source_loss()\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  for e in range(1, epochs+1):\n",
        "    train_source_loss, train_target_loss, train_accuracy = train_one_epoch(model=model,\n",
        "                                                                           source_loader=source_train_loader,\n",
        "                                                                           target_loader=target_train_loader,\n",
        "                                                                           optimizer=optimizer,\n",
        "                                                                           source_loss=cost_function,\n",
        "                                                                           entropy_loss_weights=entropy_loss_weight,\n",
        "                                                                           device=device)\n",
        "    test_loss, test_accuracy = test_one_epoch(model=model,\n",
        "                                              loader=target_test_loader,\n",
        "                                              cost_function=cost_function,\n",
        "                                              device=device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e))\n",
        "    print('\\t Train: Source loss {:.5f}, Target loss {:.2f}, Accuracy {:.2f}'.format(train_source_loss, train_target_loss, train_accuracy))\n",
        "    print('\\t Test: Source loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "HnJnGWSjfdDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aJqMBfxfd_N",
        "outputId": "b3598b8e-d27a-495c-c5f5-d2f9d58bb5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/svhn/train_32x32.mat\n",
            "Epoch: 1\n",
            "\t Train: Source loss 0.01230, Target loss 0.01, Accuracy 74.49\n",
            "\t Test: Source loss 0.00280, Accuracy 81.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Train: Source loss 0.00628, Target loss 0.00, Accuracy 87.83\n",
            "\t Test: Source loss 0.00242, Accuracy 87.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Train: Source loss 0.00523, Target loss 0.00, Accuracy 89.84\n",
            "\t Test: Source loss 0.00250, Accuracy 89.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Train: Source loss 0.00474, Target loss 0.00, Accuracy 90.82\n",
            "\t Test: Source loss 0.00196, Accuracy 90.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Train: Source loss 0.00443, Target loss 0.00, Accuracy 91.52\n",
            "\t Test: Source loss 0.00162, Accuracy 93.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Train: Source loss 0.00416, Target loss 0.00, Accuracy 92.06\n",
            "\t Test: Source loss 0.00196, Accuracy 92.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Train: Source loss 0.00399, Target loss 0.00, Accuracy 92.37\n",
            "\t Test: Source loss 0.00149, Accuracy 93.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Train: Source loss 0.00378, Target loss 0.00, Accuracy 92.92\n",
            "\t Test: Source loss 0.00209, Accuracy 92.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Train: Source loss 0.00357, Target loss 0.00, Accuracy 93.24\n",
            "\t Test: Source loss 0.00103, Accuracy 94.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Train: Source loss 0.00342, Target loss 0.00, Accuracy 93.52\n",
            "\t Test: Source loss 0.00084, Accuracy 95.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Train: Source loss 0.00331, Target loss 0.00, Accuracy 93.74\n",
            "\t Test: Source loss 0.00048, Accuracy 96.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Train: Source loss 0.00315, Target loss 0.00, Accuracy 93.97\n",
            "\t Test: Source loss 0.00055, Accuracy 96.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Train: Source loss 0.00299, Target loss 0.00, Accuracy 94.33\n",
            "\t Test: Source loss 0.00034, Accuracy 97.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Train: Source loss 0.00292, Target loss 0.00, Accuracy 94.41\n",
            "\t Test: Source loss 0.00038, Accuracy 97.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Train: Source loss 0.00280, Target loss 0.00, Accuracy 94.56\n",
            "\t Test: Source loss 0.00036, Accuracy 97.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Train: Source loss 0.00267, Target loss 0.00, Accuracy 94.86\n",
            "\t Test: Source loss 0.00040, Accuracy 97.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Train: Source loss 0.00260, Target loss 0.00, Accuracy 94.99\n",
            "\t Test: Source loss 0.00041, Accuracy 97.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Train: Source loss 0.00251, Target loss 0.00, Accuracy 95.12\n",
            "\t Test: Source loss 0.00043, Accuracy 97.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Train: Source loss 0.00247, Target loss 0.00, Accuracy 95.21\n",
            "\t Test: Source loss 0.00043, Accuracy 97.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Train: Source loss 0.00237, Target loss 0.00, Accuracy 95.31\n",
            "\t Test: Source loss 0.00043, Accuracy 97.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Train: Source loss 0.00230, Target loss 0.00, Accuracy 95.44\n",
            "\t Test: Source loss 0.00048, Accuracy 97.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Train: Source loss 0.00230, Target loss 0.00, Accuracy 95.53\n",
            "\t Test: Source loss 0.00048, Accuracy 97.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Train: Source loss 0.00225, Target loss 0.00, Accuracy 95.66\n",
            "\t Test: Source loss 0.00045, Accuracy 97.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Train: Source loss 0.00219, Target loss 0.00, Accuracy 95.66\n",
            "\t Test: Source loss 0.00041, Accuracy 97.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Train: Source loss 0.00213, Target loss 0.00, Accuracy 95.83\n",
            "\t Test: Source loss 0.00048, Accuracy 97.34\n",
            "-----------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}